{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download af structures via uniport id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1028, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene Symbol</th>\n",
       "      <th>Description</th>\n",
       "      <th>Category</th>\n",
       "      <th>Uniprot</th>\n",
       "      <th>Gifts</th>\n",
       "      <th>GC Id</th>\n",
       "      <th>Relevance score</th>\n",
       "      <th>GeneCards Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAP1LC3B</td>\n",
       "      <td>Microtubule Associated Protein 1 Light Chain 3...</td>\n",
       "      <td>Protein Coding</td>\n",
       "      <td>Q9GZQ8</td>\n",
       "      <td>54</td>\n",
       "      <td>GC16P130587</td>\n",
       "      <td>7.779193</td>\n",
       "      <td>https://www.genecards.org/cgi-bin/carddisp.pl?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATG5</td>\n",
       "      <td>Autophagy Related 5</td>\n",
       "      <td>Protein Coding</td>\n",
       "      <td>Q9H1Y0</td>\n",
       "      <td>58</td>\n",
       "      <td>GC06M106045</td>\n",
       "      <td>7.700545</td>\n",
       "      <td>https://www.genecards.org/cgi-bin/carddisp.pl?...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gene Symbol                                        Description  \\\n",
       "0    MAP1LC3B  Microtubule Associated Protein 1 Light Chain 3...   \n",
       "1        ATG5                                Autophagy Related 5   \n",
       "\n",
       "         Category Uniprot  Gifts        GC Id  Relevance score  \\\n",
       "0  Protein Coding  Q9GZQ8     54  GC16P130587         7.779193   \n",
       "1  Protein Coding  Q9H1Y0     58  GC06M106045         7.700545   \n",
       "\n",
       "                                      GeneCards Link  \n",
       "0  https://www.genecards.org/cgi-bin/carddisp.pl?...  \n",
       "1  https://www.genecards.org/cgi-bin/carddisp.pl?...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = pd.read_csv('Autophagy OR Mitophagy.csv')\n",
    "print(file.shape)\n",
    "file = file.rename(columns={'Uniprot ID':'Uniprot'})\n",
    "file.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 96/1028 [00:07<01:09, 13.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download Q13315: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-Q13315-F1-model_v6.pdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 134/1028 [00:10<01:10, 12.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download O95613: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-O95613-F1-model_v6.pdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 206/1028 [00:16<00:49, 16.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download Q14204: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-Q14204-F1-model_v6.pdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 347/1028 [00:28<00:53, 12.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download Q03164: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-Q03164-F1-model_v6.pdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 355/1028 [00:28<00:50, 13.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download P04114: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-P04114-F1-model_v6.pdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 474/1028 [00:39<00:45, 12.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download Q9NR09: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-Q9NR09-F1-model_v6.pdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 650/1028 [00:55<00:28, 13.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download nan: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-nan-F1-model_v6.pdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 670/1028 [00:56<00:30, 11.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download Q8IZQ1: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-Q8IZQ1-F1-model_v6.pdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 705/1028 [00:59<00:24, 13.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download Q5T4S7: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-Q5T4S7-F1-model_v6.pdb\n",
      "Failed to download Q15751: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-Q15751-F1-model_v6.pdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 729/1028 [01:01<00:26, 11.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download Q7Z6Z7: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-Q7Z6Z7-F1-model_v6.pdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 735/1028 [01:02<00:23, 12.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download P42858: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-P42858-F1-model_v6.pdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 743/1028 [01:03<00:24, 11.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download P50851: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-P50851-F1-model_v6.pdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 751/1028 [01:03<00:19, 14.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download Q5THJ4: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-Q5THJ4-F1-model_v6.pdb\n",
      "Failed to download Q709C8: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-Q709C8-F1-model_v6.pdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 767/1028 [01:05<00:20, 12.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download Q14643: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-Q14643-F1-model_v6.pdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 845/1028 [01:11<00:14, 12.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download Q96RL7: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-Q96RL7-F1-model_v6.pdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 931/1028 [01:19<00:08, 11.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download Q6ZS81: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-Q6ZS81-F1-model_v6.pdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 970/1028 [01:22<00:03, 16.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download nan: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-nan-F1-model_v6.pdb\n",
      "Failed to download nan: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-nan-F1-model_v6.pdb\n",
      "Failed to download nan: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-nan-F1-model_v6.pdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 992/1028 [01:24<00:02, 12.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download P78527: 404 Client Error: Not Found for url: https://alphafold.ebi.ac.uk/files/AF-P78527-F1-model_v6.pdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1028/1028 [01:27<00:00, 11.76it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "# 假设 mouse_ids.txt 存放在同一目录\n",
    "ids = file.Uniprot.tolist()\n",
    "error_list = []\n",
    "output_dir = Path(\"alphafold_mouse\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "for uniprot_id in tqdm(ids):\n",
    "    url = f\"https://alphafold.ebi.ac.uk/files/AF-{uniprot_id}-F1-model_v6.pdb\"\n",
    "    out_path = output_dir / f\"AF-{uniprot_id}-F1-model_v4.pdb\"\n",
    "    #print(f\"Downloading {uniprot_id} ...\")\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        out_path.write_bytes(r.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {uniprot_id}: {e}\")\n",
    "        error_list.append(uniprot_id)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = file.Uniprot.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1006, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AF-P01116-F1-model_v4.pdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AF-Q96DM3-F1-model_v4.pdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AF-Q7L5A8-F1-model_v4.pdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AF-P51153-F1-model_v4.pdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AF-Q8TC71-F1-model_v4.pdb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    filename\n",
       "0  AF-P01116-F1-model_v4.pdb\n",
       "1  AF-Q96DM3-F1-model_v4.pdb\n",
       "2  AF-Q7L5A8-F1-model_v4.pdb\n",
       "3  AF-P51153-F1-model_v4.pdb\n",
       "4  AF-Q8TC71-F1-model_v4.pdb"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 指定目录路径\n",
    "dir_path = 'alphafold_mouse'\n",
    "\n",
    "# 获取该目录下的所有文件名（非目录）\n",
    "filenames = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n",
    "\n",
    "# 创建 DataFrame\n",
    "df = pd.DataFrame(filenames, columns=['filename'])\n",
    "print(df.shape)\n",
    "# 查看前几行\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1028, 1)\n",
      "(22, 1)\n"
     ]
    }
   ],
   "source": [
    "df['name'] = df[\"filename\"].str.split(\"-\").str[1]\n",
    "ids_df = pd.DataFrame(ids, columns=[\"name\"])\n",
    "\n",
    "print(ids_df.shape)\n",
    "ids_df = ids_df[~ids_df.name.isin(df['name'])]\n",
    "print(ids_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('downloaded_AFpdb.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_df.to_csv('missing_AFpdb.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pocket prediction and fliter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use p2rank to seg pockets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv('downloaded_AFpdb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = f\"protein_list.ds\"\n",
    "with open(ds, \"w\") as out:\n",
    "    for i in df.filename:\n",
    "        out.write(f\"alphafold_mouse/{i}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.unpack_archive('p2rank_23_archive.zip', 'p2rank_23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "start_time = time.time()\n",
    "p2rank = \"p2rank_23/prank\"\n",
    "cmd = f\"{p2rank} predict {ds} -o af_workspace1/p2rank -threads 64\"\n",
    "os.system(cmd)\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "p2rank_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360.1603763103485\n"
     ]
    }
   ],
   "source": [
    "print(p2rank_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fliter none pocket proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    center_x  center_y  center_z\n",
      "0   -18.8499  -18.7102  -14.5548\n",
      "1    -9.4584   -5.3011  -26.8683\n",
      "2   -22.5981   -3.3566   -9.2237\n",
      "3    -5.3422  -11.3895  -10.7987\n",
      "4    -7.2313   33.7364   25.2334\n",
      "5     2.4678   26.1263    5.1129\n",
      "6    10.7386   13.8563    8.5858\n",
      "7   -25.9379   -6.6773    6.3810\n",
      "8   -13.4745   -8.6405    0.5189\n",
      "9    -0.2896   -0.5392  -17.2942\n",
      "10  -32.9533  -12.8549  -10.1700\n",
      "11  -24.2033  -60.4937   39.2404\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>probability</th>\n",
       "      <th>sas_points</th>\n",
       "      <th>surf_atoms</th>\n",
       "      <th>center_x</th>\n",
       "      <th>center_y</th>\n",
       "      <th>center_z</th>\n",
       "      <th>residue_ids</th>\n",
       "      <th>surf_atom_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pocket1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.26</td>\n",
       "      <td>0.473</td>\n",
       "      <td>83</td>\n",
       "      <td>45</td>\n",
       "      <td>-18.8499</td>\n",
       "      <td>-18.7102</td>\n",
       "      <td>-14.5548</td>\n",
       "      <td>A_216 A_218 A_269 A_271 A_272 A_296 A_298 A_3...</td>\n",
       "      <td>1618 1634 1998 2008 2009 2015 2016 2018 2199 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pocket2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.58</td>\n",
       "      <td>0.190</td>\n",
       "      <td>49</td>\n",
       "      <td>22</td>\n",
       "      <td>-9.4584</td>\n",
       "      <td>-5.3011</td>\n",
       "      <td>-26.8683</td>\n",
       "      <td>A_221 A_226 A_227 A_228 A_229 A_230 A_231 A_2...</td>\n",
       "      <td>1658 1687 1688 1691 1696 1697 1699 1707 1715 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name         rank     score   probability   sas_points   surf_atoms  \\\n",
       "0  pocket1         1     12.26         0.473           83           45   \n",
       "1  pocket2         2      6.58         0.190           49           22   \n",
       "\n",
       "      center_x     center_y     center_z  \\\n",
       "0     -18.8499     -18.7102     -14.5548   \n",
       "1      -9.4584      -5.3011     -26.8683   \n",
       "\n",
       "                                         residue_ids  \\\n",
       "0   A_216 A_218 A_269 A_271 A_272 A_296 A_298 A_3...   \n",
       "1   A_221 A_226 A_227 A_228 A_229 A_230 A_231 A_2...   \n",
       "\n",
       "                                       surf_atom_ids  \n",
       "0   1618 1634 1998 2008 2009 2015 2016 2018 2199 ...  \n",
       "1   1658 1687 1688 1691 1696 1697 1699 1707 1715 ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tmp = pd.read_csv(\"af_workspace/p2rank/AF-Q14005-F1-model_v4.pdb_predictions.csv\")\n",
    "print(tmp[['   center_x', '   center_y', '   center_z']])\n",
    "tmp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking files: 100%|██████████| 1006/1006 [00:03<00:00, 263.78file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 914 non-empty files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm  # 导入tqdm\n",
    "\n",
    "workspace = \"af_workspace/p2rank\"\n",
    "non_empty_ids = []\n",
    "\n",
    "# 获取所有匹配的文件名并排序（可选，使进度条更清晰）\n",
    "matching_files = [f for f in os.listdir(workspace) if re.match(r'AF-.+-F1-model_v4\\.pdb_predictions\\.csv', f)]\n",
    "matching_files.sort()  # 按文件名排序\n",
    "\n",
    "# 使用tqdm包装文件遍历循环\n",
    "for filename in tqdm(matching_files, desc=\"Checking files\", unit=\"file\"):\n",
    "    match = re.match(r'AF-(.+)-F1-model_v4\\.pdb_predictions\\.csv', filename)\n",
    "    if match:\n",
    "        file_id = match.group(1)\n",
    "        file_path = os.path.join(workspace, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            if len(df) > 0:\n",
    "                non_empty_ids.append(file_id)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            continue\n",
    "\n",
    "result_df = pd.DataFrame({\"non_empty_ids\": non_empty_ids})\n",
    "print(f\"Found {len(non_empty_ids)} non-empty files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('downloaded_AFpdb_wpocket.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample code for docking with predicted pocket info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_df = pd.read_csv('downloaded_AFpdb_wpocket.csv')\n",
    "df  = pd.read_csv('downloaded_AFpdb.csv')\n",
    "df = df[df.name.isin(result_df.non_empty_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [07:21<00:00,  2.07it/s]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pdbfixer import PDBFixer\n",
    "from openmm.app import PDBFile\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "def add_h(protein_file, outpath):\n",
    "    try:\n",
    "        fixer = PDBFixer(protein_file)\n",
    "        fixer.findMissingResidues()\n",
    "        fixer.findNonstandardResidues()\n",
    "        fixer.replaceNonstandardResidues()\n",
    "        fixer.removeHeterogens(True)\n",
    "        fixer.findMissingAtoms()\n",
    "        fixer.addMissingAtoms()\n",
    "        fixer.addMissingHydrogens(7.4)\n",
    "\n",
    "        with open(outpath, \"w\") as f:\n",
    "            PDBFile.writeFile(fixer.topology, fixer.positions, f)\n",
    "        return (protein_file, True, \"\")\n",
    "    except Exception as e:\n",
    "        return (protein_file, False, str(e))\n",
    "def process_file(filename, prep_dir, out_dir):\n",
    "    protein_path = f\"{prep_dir}/{filename}\"\n",
    "    outpath = f\"{out_dir}/{filename}\"\n",
    "    return add_h(protein_path, outpath)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    outpath = 'alphafold_mouse'\n",
    "    output_dir = Path(outpath)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    prep = 'alphafold_mouse'\n",
    "    \n",
    "    filenames = df.filename.tolist()\n",
    "\n",
    "    # 并行处理\n",
    "    results = []\n",
    "    with ProcessPoolExecutor(max_workers=8) as executor:\n",
    "        futures = [executor.submit(process_file, fname, prep, outpath) for fname in filenames]\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "            if not result[1]:\n",
    "                print(f\"❌ Failed: {result[0]}\\nError: {result[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "outpath = 'debug'\n",
    "output_dir = Path(outpath)\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "ligand_o = f'af_workspace/Keto.sdf'\n",
    "ligand_h = f'af_workspace/NAce.sdf'\n",
    "prep='alphafold_mouse'\n",
    "for i in tqdm(df.filename):\n",
    "    try:\n",
    "        df1 = pd.read_csv(f'af_workspace/p2rank/{i}_predictions.csv')\n",
    "        protein_path = f'{prep}/{i}'\n",
    "        i=i.split(\"-\")[1]\n",
    "        print(f'start docking {i}')\n",
    "        out_dict_o = f'{outpath}/{i}/'\n",
    "        out_dict_h = f'{outpath}/{i}/'\n",
    "        #add_h(protein_path, protein_path)\n",
    "        '''for j in range(df.shape[0]):\n",
    "            x,y,z = df1.loc[j][['   center_x', '   center_y', '   center_z']].values\n",
    "            subprocess.run(f'unidocktools unidock_pipeline -r {protein_path} -l {ligand_o} -sd {out_dict_o} -nm 10 -cx {x} -cy {y} -cz {z}', shell=True, check=True)\n",
    "            subprocess.run(f'unidocktools unidock_pipeline -r {protein_path} -l {ligand_h} -sd {out_dict_h} -nm 10 -cx {x} -cy {y} -cz {z}', shell=True, check=True)\n",
    "        '''\n",
    "        x,y,z  = df1['   center_x'][0],df1['   center_y'][0],df1['   center_z'][0]\n",
    "        subprocess.run(f'unidocktools unidock_pipeline -r {protein_path} -l {ligand_o} -sd {out_dict_o} -nm 25 -cx {x} -cy {y} -cz {z}', shell=True, check=True)\n",
    "        subprocess.run(f'unidocktools unidock_pipeline -r {protein_path} -l  {ligand_h} -sd {out_dict_h} -nm 25 -cx {x} -cy {y} -cz {z}', shell=True, check=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting docking info: 100%|██████████| 914/914 [00:38<00:00, 23.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def filter_pdb(input_file, output_file=None):\n",
    "    # 如果未提供输出文件名，使用与输入文件相同目录的临时文件\n",
    "    if output_file is None or input_file == output_file:\n",
    "        # 获取输入文件所在目录\n",
    "        input_dir = os.path.dirname(os.path.abspath(input_file))\n",
    "        # 在相同目录创建临时文件\n",
    "        temp_fd, temp_path = tempfile.mkstemp(\n",
    "            suffix='.pdb', \n",
    "            dir=input_dir\n",
    "        )\n",
    "        os.close(temp_fd)\n",
    "    else:\n",
    "        temp_path = None\n",
    "    \n",
    "    try:\n",
    "        with open(input_file, 'r') as infile, open(temp_path or output_file, 'w') as outfile:\n",
    "            for line in infile:\n",
    "                if line.startswith(('ATOM', 'TER', 'END')): #, 'CONECT'\n",
    "                    outfile.write(line)\n",
    "        \n",
    "        # 如果使用了临时文件，替换原文件\n",
    "        if temp_path:\n",
    "            os.replace(temp_path, input_file)  # 现在在同一文件系统内，不会报错\n",
    "            #print(f\"已安全更新文件: {input_file}\")\n",
    "    \n",
    "    finally:\n",
    "        # 确保清理临时文件（即使出错）\n",
    "        if temp_path and os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "# 遍历每个子文件夹（每个蛋白）\n",
    "for subdir in tqdm(list(docking_dir.iterdir()), desc=\"Collecting docking info\"):\n",
    "    if subdir.is_dir():\n",
    "        pdb_id = subdir.name\n",
    "        protein_file = protein_dir / f\"AF-{pdb_id}-F1-model_v4.pdb\"\n",
    "        #print(protein_file)\n",
    "        if not protein_file.exists():\n",
    "            print(f\"[Warning] Missing protein file: {protein_file}\")\n",
    "            continue\n",
    "            \n",
    "        filter_pdb(protein_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare ligand config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1756, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source_idx</th>\n",
       "      <th>Smiles</th>\n",
       "      <th>target_idx</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>reference_compound_name</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107778</td>\n",
       "      <td>CS(=O)CCCCCCCCN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.658061</td>\n",
       "      <td>CS(=O)CCCCN=C=S</td>\n",
       "      <td>af_workspace/107778.sdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Source_idx           Smiles  target_idx  similarity_score  \\\n",
       "0     107778  CS(=O)CCCCCCCCN           0          0.658061   \n",
       "\n",
       "  reference_compound_name                     path  \n",
       "0         CS(=O)CCCCN=C=S  af_workspace/107778.sdf  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ligand = pd.read_csv('mTor_independent_compound.csv')\n",
    "ligand['Source_idx'] = ligand['Source_idx'].astype(str)\n",
    "ligand['path'] = \"af_workspace/\"+ligand.Source_idx+'.sdf'\n",
    "print(ligand.shape)\n",
    "ligand.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.88it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, SDWriter\n",
    "from tqdm import tqdm\n",
    "def smiles_to_individual_sdf(df, smiles_col, name_col, out_dir=\"./sdf_files\"):\n",
    "    \"\"\"\n",
    "    将 DataFrame 里的 SMILES 转换为分子，每个分子保存为单独的 SDF 文件\n",
    "    文件名由 name_col 决定\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        smi = row[smiles_col]\n",
    "        name = str(row[name_col])  # 用 name_col 来命名\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None:\n",
    "            print(smi)\n",
    "            continue\n",
    "\n",
    "        # 加氢、生成 3D 构象\n",
    "        mol = Chem.AddHs(mol)\n",
    "        AllChem.EmbedMolecule(mol, AllChem.ETKDG())\n",
    "        AllChem.UFFOptimizeMolecule(mol)\n",
    "\n",
    "        # 去氢\n",
    "        mol_noH = Chem.RemoveHs(mol)\n",
    "\n",
    "        # 设置分子名称（会写到 SDF title line）\n",
    "        mol_noH.SetProp(\"_Name\", name)\n",
    "\n",
    "        # 保存为单独文件\n",
    "        file_path = os.path.join(out_dir, f\"{name}.sdf\")\n",
    "        with SDWriter(file_path) as writer:\n",
    "            writer.write(mol_noH)\n",
    "\n",
    "        #print(f\"✅ {name}.sdf 已保存\")\n",
    "\n",
    "\n",
    "\n",
    "smiles_to_individual_sdf(ligand, smiles_col=\"Smiles\", name_col=\"Source_idx\", out_dir=\"./af_workspace\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# 指定你的文件夹路径\n",
    "folder = \"af_workspace\"\n",
    "# 获取所有 .sdf 文件名\n",
    "sdf_files = [f for f in os.listdir(folder) if f.endswith(\".sdf\")]\n",
    "# 转换成 DataFrame\n",
    "df1 = pd.DataFrame({\"ligand\": sdf_files})\n",
    "df1['ligand'] = \"af_workspace/\"+df1['ligand']\n",
    "df1.to_csv('ligands.csv',index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-Dock v1.1.1\n",
      "\n",
      "If you used Uni-Dock in your work, please cite:               \n",
      " \n",
      "Yu, Y., Cai, C., Wang, J., Bo, Z., Zhu, Z., & Zheng, H. (2023). \n",
      "Uni-Dock: GPU-Accelerated Docking Enables Ultralarge Virtual Screening. \n",
      "Journal of Chemical Theory and Computation.                    \n",
      "https://doi.org/10.1021/acs.jctc.2c01145                       \n",
      "\n",
      "Tang, S., Chen, R., Lin, M., Lin, Q., Zhu, Y., Ding, J., ... & Wu, J. (2022). \n",
      "Accelerating autodock vina with gpus. Molecules, 27(9), 3041. \n",
      "DOI 10.3390/molecules27093041                                 \n",
      "\n",
      "J. Eberhardt, D. Santos-Martins, A. F. Tillack, and S. Forli  \n",
      "AutoDock Vina 1.2.0: New Docking Methods, Expanded Force      \n",
      "Field, and Python Bindings, J. Chem. Inf. Model. (2021)       \n",
      "DOI 10.1021/acs.jcim.1c00203                                  \n",
      "\n",
      "O. Trott, A. J. Olson,                                        \n",
      "AutoDock Vina: improving the speed and accuracy of docking    \n",
      "with a new scoring function, efficient optimization and        \n",
      "multithreading, J. Comp. Chem. (2010)                         \n",
      "DOI 10.1002/jcc.21334                                         \n",
      "\n",
      "Please refer to https://github.com/dptech-corp/Uni-Dock/ for  \n",
      "bug reporting, license agreements, and more information.      \n",
      "\n",
      "Output will be 20883_out.sdf\n",
      "Scoring function : vina\n",
      "Rigid receptor: alphafold_mouse/AF-A1A4Y4-F1-model_v4.pdb\n",
      "Ligand: 20883.sdf\n",
      "Grid center: X 5.4514 Y -5.9074 Z -10.3422\n",
      "Grid size  : X 25 Y 25 Z 25\n",
      "Grid space : 0.375\n",
      "Exhaustiveness: 384\n",
      "CPU: 0\n",
      "Verbosity: 1\n",
      "\n",
      "done.ting Vina grid ... \n",
      "angle_to_quaternion axis.norm=nan, failed641) ... \n",
      "angle_to_quaternion axis.norm=nan, failed\n",
      "angle_to_quaternion axis.norm=nan, failed\n",
      "angle_to_quaternion axis.norm=nan, failed\n",
      "angle_to_quaternion axis.norm=nan, failed\n",
      "angle_to_quaternion axis.norm=nan, failed\n",
      "angle_to_quaternion axis.norm=nan, failed\n",
      "angle_to_quaternion axis.norm=nan, failed\n",
      "angle_to_quaternion axis.norm=nan, failed\n",
      "angle_to_quaternion axis.norm=nan, failed\n",
      "angle_to_quaternion axis.norm=nan, failed\n",
      "angle_to_quaternion axis.norm=nan, failed\n",
      "angle_to_quaternion axis.norm=nan, failed\n",
      "angle_to_quaternion axis.norm=nan, failed\n",
      "angle_to_quaternion axis.norm=nan, failed\n",
      "angle_to_quaternion axis.norm=nan, failed\n",
      "angle_to_quaternion axis.norm=nan, failed\n",
      "angle_to_quaternion axis.norm=nan, failed\n",
      "angle_to_quaternion axis.norm=nan, failed\n",
      "angle_to_quaternion axis.norm=nan, failed\n",
      "\n",
      "mode |   affinity | dist from best mode\n",
      "     | (kcal/mol) | rmsd l.b.| rmsd u.b.\n",
      "-----+------------+----------+----------\n",
      "   1       -6.123          0          0\n",
      "   2       -5.872      5.797      9.415\n",
      "   3       -5.829      2.996      6.289\n",
      "   4       -5.816       2.47      7.814\n",
      "   5       -5.813      4.902      8.128\n",
      "   6       -5.702      3.005       7.96\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "import time\n",
    "start = time.time()\n",
    "file_path = 'alphafold_mouse/AF-O60356-F1-model_v4.pdb'\n",
    "df1 = pd.read_csv(f'af_workspace1/p2rank/AF-A1A4Y4-F1-model_v4.pdb_predictions.csv')\n",
    "df1.columns = [col.strip() for col in df1.columns]\n",
    "x, y, z = df1.loc[0][['center_x', 'center_y', 'center_z']].values\n",
    "x, y, z\n",
    "subprocess.run(f'unidock --receptor alphafold_mouse/AF-A1A4Y4-F1-model_v4.pdb --ligand 20883.sdf --search_mode balance --center_x {x} --center_y {y} --center_z {z} --size_x 25 --size_y 25 --size_z 25 --dir tmp', shell=True, check=True)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(df.shape[0])):#\n",
    "    file_path = 'alphafold_mouse/'+df.filename.iloc[i]\n",
    "    df1 = pd.read_csv(f'af_workspace/p2rank/{df.filename.iloc[i]}_predictions.csv')\n",
    "    df1.columns = [col.strip() for col in df1.columns]\n",
    "    x, y, z = df1.loc[0][['center_x', 'center_y', 'center_z']].values\n",
    "\n",
    "    \n",
    "    outputdir = f\"dock_record/{file_path.split('/')[-1][:-4]}\"\n",
    "    os.makedirs(outputdir, exist_ok=True)\n",
    "    subprocess.run(f'unidock --receptor {file_path} --ligand_index ligands.csv --search_mode balance --center_x {x} --center_y {y} --center_z {z} --size_x 25 --size_y 25 --size_z 25 --dir {outputdir}', shell=True, check=True)\n",
    "    \n",
    "    base_path = './'\n",
    "    output_folder = os.path.join(base_path, \"files\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    save_name = f\"{file_path.split('/')[-1][:-5]}.txt\"\n",
    "    file_save_path = os.path.join(output_folder, save_name)\n",
    "    # 保存到 txt\n",
    "    with open(file_save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write('finished')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 根目录（包含多个 AF-... 文件夹）\n",
    "base_dir = \"dock_record\"\n",
    "\n",
    "def get_target_folders(base_dir):\n",
    "    pattern = os.path.join(base_dir, \"AF-*\")\n",
    "    folders = [f for f in glob.glob(pattern) if os.path.isdir(f)]\n",
    "    return folders\n",
    "\n",
    "def process_folder(folder):\n",
    "    results = []\n",
    "    folder_name = os.path.basename(folder)\n",
    "    try:\n",
    "        protein_id = folder_name.split('-')[1]  # e.g., Q92985\n",
    "    except IndexError:\n",
    "        protein_id = \"UNKNOWN\"\n",
    "    \n",
    "    sdf_files = glob.glob(os.path.join(folder, \"*.sdf\"))\n",
    "    for sdf_file in sdf_files:\n",
    "        try:\n",
    "            with open(sdf_file, \"r\") as f:\n",
    "                content = f.read()\n",
    "        except Exception as e:\n",
    "            continue  # silent skip or log if needed\n",
    "\n",
    "        blocks = content.split(\"$$$$\")\n",
    "        for block in blocks:\n",
    "            match = re.search(r\"> <Uni-Dock RESULT>.*?ENERGY=\\s*([-\\d\\.]+)\", block, re.DOTALL)\n",
    "            if match:\n",
    "                try:\n",
    "                    energy = float(match.group(1))\n",
    "                    results.append({\n",
    "                        \"file\": os.path.basename(sdf_file),\n",
    "                        \"energy\": energy,\n",
    "                        \"protein\": protein_id,\n",
    "                        \"folder\": folder_name\n",
    "                    })\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                break  # 只取第一个 ENERGY\n",
    "\n",
    "    if not results:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df = df.sort_values(by=\"energy\", ascending=True).reset_index(drop=True)\n",
    "    df = df.drop_duplicates(subset=[\"file\", \"protein\"], keep=\"first\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    folders = get_target_folders(base_dir)\n",
    "    print(f\"Found {len(folders)} folders to process.\")\n",
    "    if not folders:\n",
    "        print(\"No folders found. Check base_dir path.\")\n",
    "        return\n",
    "\n",
    "    all_dfs = []\n",
    "    with ProcessPoolExecutor(max_workers=min(16, os.cpu_count())) as executor:\n",
    "        # 提交所有任务\n",
    "        future_to_folder = {executor.submit(process_folder, folder): folder for folder in folders}\n",
    "        \n",
    "        # 使用 tqdm 显示进度\n",
    "        with tqdm(total=len(future_to_folder), desc=\"Processing folders\") as pbar:\n",
    "            for future in as_completed(future_to_folder):\n",
    "                try:\n",
    "                    df = future.result()\n",
    "                    if not df.empty:\n",
    "                        all_dfs.append(df)\n",
    "                except Exception as e:\n",
    "                    folder = future_to_folder[future]\n",
    "                    print(f\"\\nError processing {folder}: {e}\")\n",
    "                finally:\n",
    "                    pbar.update(1)  # 手动更新进度\n",
    "\n",
    "    if all_dfs:\n",
    "        final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        final_df.to_csv(\"merged_docking_results1.csv\", index=False)\n",
    "        print(f\"\\n✅ Saved merged results to merged_docking_results.csv (total rows: {len(final_df)})\")\n",
    "    else:\n",
    "        print(\"\\n❌ No valid results found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    final_df = pd.read_csv('merged_docking_results1.csv')\n",
    "    final_df['ligand'] = final_df.file.str[:-8].astype('int')\n",
    "    final_df = final_df[final_df['ligand']<30]\n",
    "    final_df[['energy', 'protein', 'ligand']].to_csv('merged_docking_results.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
